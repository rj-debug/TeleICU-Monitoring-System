{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOCTOR DETECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 doctors, 203.7ms\n",
      "Speed: 17.4ms preprocess, 203.7ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 doctorss, 139.0ms\n",
      "Speed: 6.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 doctorss, 136.3ms\n",
      "Speed: 3.0ms preprocess, 136.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 144.4ms\n",
      "Speed: 2.0ms preprocess, 144.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 136.8ms\n",
      "Speed: 2.5ms preprocess, 136.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 142.9ms\n",
      "Speed: 1.0ms preprocess, 142.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 137.5ms\n",
      "Speed: 2.0ms preprocess, 137.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 138.9ms\n",
      "Speed: 2.0ms preprocess, 138.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 139.6ms\n",
      "Speed: 2.0ms preprocess, 139.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 148.1ms\n",
      "Speed: 2.0ms preprocess, 148.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 151.2ms\n",
      "Speed: 2.0ms preprocess, 151.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 144.7ms\n",
      "Speed: 1.0ms preprocess, 144.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 150.9ms\n",
      "Speed: 2.5ms preprocess, 150.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 152.7ms\n",
      "Speed: 2.0ms preprocess, 152.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 doctorss, 149.6ms\n",
      "Speed: 3.0ms preprocess, 149.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 138.4ms\n",
      "Speed: 2.0ms preprocess, 138.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 141.6ms\n",
      "Speed: 2.5ms preprocess, 141.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 139.0ms\n",
      "Speed: 3.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 137.8ms\n",
      "Speed: 2.0ms preprocess, 137.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 138.8ms\n",
      "Speed: 2.0ms preprocess, 138.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 133.5ms\n",
      "Speed: 3.5ms preprocess, 133.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 132.9ms\n",
      "Speed: 2.0ms preprocess, 132.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 134.0ms\n",
      "Speed: 3.5ms preprocess, 134.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 138.3ms\n",
      "Speed: 3.0ms preprocess, 138.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 139.2ms\n",
      "Speed: 1.0ms preprocess, 139.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 169.5ms\n",
      "Speed: 2.0ms preprocess, 169.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 170.1ms\n",
      "Speed: 2.3ms preprocess, 170.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 141.6ms\n",
      "Speed: 2.7ms preprocess, 141.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 doctorss, 137.8ms\n",
      "Speed: 2.0ms preprocess, 137.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 137.4ms\n",
      "Speed: 2.0ms preprocess, 137.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 142.5ms\n",
      "Speed: 2.0ms preprocess, 142.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 135.4ms\n",
      "Speed: 3.5ms preprocess, 135.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 142.2ms\n",
      "Speed: 2.0ms preprocess, 142.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 141.4ms\n",
      "Speed: 2.0ms preprocess, 141.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 139.3ms\n",
      "Speed: 2.0ms preprocess, 139.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 139.1ms\n",
      "Speed: 2.0ms preprocess, 139.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 140.3ms\n",
      "Speed: 1.5ms preprocess, 140.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 139.0ms\n",
      "Speed: 1.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 136.7ms\n",
      "Speed: 2.0ms preprocess, 136.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 145.9ms\n",
      "Speed: 2.5ms preprocess, 145.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 150.2ms\n",
      "Speed: 1.0ms preprocess, 150.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 151.6ms\n",
      "Speed: 3.5ms preprocess, 151.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 138.3ms\n",
      "Speed: 2.5ms preprocess, 138.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 138.0ms\n",
      "Speed: 1.0ms preprocess, 138.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 139.2ms\n",
      "Speed: 2.1ms preprocess, 139.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 144.3ms\n",
      "Speed: 2.5ms preprocess, 144.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 137.9ms\n",
      "Speed: 1.5ms preprocess, 137.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 138.4ms\n",
      "Speed: 2.0ms preprocess, 138.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 140.8ms\n",
      "Speed: 2.0ms preprocess, 140.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 140.5ms\n",
      "Speed: 3.0ms preprocess, 140.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 doctors, 146.7ms\n",
      "Speed: 2.0ms preprocess, 146.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('best_small.pt')\n",
    "\n",
    "# Define the input and output video paths\n",
    "input_video_path = 'vids/Doc.mp4'\n",
    "output_video_path = 'output_video_doc.avi'\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get the video writer initialized to save the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    if frame_count % 3 != 0:\n",
    "        # Skip frames that are not the third frame\n",
    "        continue\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(frame)\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy  # Bounding box coordinates\n",
    "        confs = result.boxes.conf  # Confidence scores\n",
    "        classes = result.boxes.cls  # Class labels\n",
    "\n",
    "        for box, conf, cls in zip(boxes, confs, classes):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            label = int(cls)\n",
    "            confidence = float(conf)\n",
    "\n",
    "            # Draw the bounding box and label on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'{label} {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VITAL SIGN DETECTION - (EXPERIMENTAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x416 1 object, 51.4ms\n",
      "Speed: 4.0ms preprocess, 51.4ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 416)\n",
      "\n",
      "0: 416x416 1 HR, 1 SPO2, 59.2ms\n",
      "Speed: 2.5ms preprocess, 59.2ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 416)\n",
      "Detected Heart Rate: 76\n",
      "HR: 76\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize YOLO models\n",
    "general_model = YOLO('best_Monitor.pt')  # General object detection model\n",
    "vitals_model = YOLO('best_vitals-2.pt')  # Your custom model for vitals detection\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Define the threshold for low heart rate\n",
    "LOW_HR_THRESHOLD = 60\n",
    "HIGH_HR_THRESHOLD = 150\n",
    "null_val='_?_'\n",
    "\n",
    "def crop_roi(frame, results):\n",
    "    for result in results:\n",
    "        boxes = result.boxes.cpu().numpy()\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            return frame[y1:y2, x1:x2]\n",
    "    return frame  # Return full frame if no detection\n",
    "\n",
    "def detect_vitals(roi):\n",
    "    results = vitals_model(roi)\n",
    "    vitals_boxes = {}\n",
    "    for result in results:\n",
    "        boxes = result.boxes.cpu().numpy()\n",
    "        for box in boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = vitals_model.names[class_id]\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            vitals_boxes[class_name] = (x1, y1, x2, y2)\n",
    "    return vitals_boxes\n",
    "\n",
    "def process_frame(frame):\n",
    "    # Use general YOLO model to detect the monitor\n",
    "    general_results = general_model(frame)\n",
    "    \n",
    "    # Crop the frame to the detected monitor ROI\n",
    "    monitor_roi = crop_roi(frame, general_results)\n",
    "    \n",
    "    # Detect vitals within the monitor ROI\n",
    "    vitals_boxes = detect_vitals(monitor_roi)\n",
    "    \n",
    "    # Check if HR (heart rate) is detected\n",
    "    if 'HR' in vitals_boxes:\n",
    "        x1, y1, x2, y2 = vitals_boxes['HR']\n",
    "        hr_roi = monitor_roi[y1:y2, x1:x2]\n",
    "        \n",
    "        # Use EasyOCR to extract text from the HR ROI\n",
    "        grey_img = cv2.cvtColor(hr_roi, cv2.COLOR_RGB2GRAY)\n",
    "        ocr_result = reader.readtext(grey_img)\n",
    "        \n",
    "        # Extract the first number encountered\n",
    "        return extract_first_number(ocr_result), hr_roi\n",
    "    \n",
    "    return None, monitor_roi\n",
    "\n",
    "def extract_first_number(ocr_result):\n",
    "    for detection in ocr_result:\n",
    "        text = detection[1]  # The recognized text is the second element of each detection\n",
    "        try:\n",
    "            return int(text)\n",
    "        except ValueError:\n",
    "            continue  # If it's not a number, continue to the next detection\n",
    "    return None\n",
    "\n",
    "def check_low_heart_rate(heart_rate):\n",
    "    if heart_rate < LOW_HR_THRESHOLD or heart_rate > HIGH_HR_THRESHOLD or heart_rate == null_val:\n",
    "        print(f\"ALERT: Low heart rate detected! Current rate: {heart_rate} bpm\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    # Ask user for image file path\n",
    "    image_path = 'vids/vitals-1.jpg'\n",
    "    \n",
    "    # Read the image\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        print(\"Error opening image file\")\n",
    "        return\n",
    "    \n",
    "    heart_rate, roi = process_frame(frame)\n",
    "    \n",
    "    if heart_rate is not None:\n",
    "        print(f\"Detected Heart Rate: {heart_rate}\")\n",
    "        \n",
    "        # Check for low heart rate and send alert if necessary\n",
    "        if check_low_heart_rate(heart_rate):\n",
    "            print(\"LOW HR ALERT!\")\n",
    "        \n",
    "        print(f\"HR: {heart_rate}\")\n",
    "    else:\n",
    "        print(\"No HR detected\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FALL DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path='tflite-model-maker-falldetect-model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Get the input shape\n",
    "input_shape = input_details[0]['shape']\n",
    "height = input_shape[1]\n",
    "width = input_shape[2]\n",
    "\n",
    "# Open the video stream (use 0 for webcam, or provide a video file path)\n",
    "cap = cv2.VideoCapture('vids/fall.mp4')  # Use 0 for webcam, or 'path_to_your_video.mp4' for a video file\n",
    "\n",
    "# For FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# Parameters for fall detection\n",
    "detection_buffer = deque(maxlen=100)  # Increased buffer size\n",
    "initial_fall_threshold = 0.8  # High initial threshold\n",
    "min_fall_threshold = 0.7  # Minimum threshold\n",
    "fall_frames_threshold = 90  # Number of frames required for fall detection\n",
    "cooldown_frames = 60  # Frames to wait before allowing another detection\n",
    "frames_since_last_detection = cooldown_frames  # Initialize as if we just had a detection\n",
    "\n",
    "fall_detected = False\n",
    "fall_start_time = None\n",
    "\n",
    "# Function to calculate dynamic threshold\n",
    "#def calculate_dynamic_threshold(buffer):\n",
    "#    if len(buffer) < 20:\n",
    "#        return initial_fall_threshold\n",
    "#    sorted_probs = sorted(buffer)\n",
    "#    return max(min_fall_threshold, np.mean(sorted_probs[-20:]) + 2 * np.std(sorted_probs[-20:]))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Preprocess the frame\n",
    "    resized_frame = cv2.resize(frame, (width, height))\n",
    "    input_data = np.expand_dims(resized_frame, axis=0).astype(np.uint8)\n",
    "    \n",
    "    # Set the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get the output tensor\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Process the output\n",
    "    fall_probability = output_data[0][0]\n",
    "    detection_buffer.append(fall_probability)\n",
    "    \n",
    "    # Calculate dynamic threshold\n",
    "    current_threshold = 230 #calculate_dynamic_threshold(detection_buffer)\n",
    "    \n",
    "    # Check if a fall is detected\n",
    "    if frames_since_last_detection >= cooldown_frames:\n",
    "        if fall_probability > current_threshold:\n",
    "            fall_detected = True\n",
    "            fall_start_time = time.time()\n",
    "            frames_since_last_detection = 0\n",
    "    else:\n",
    "        frames_since_last_detection += 1\n",
    "    \n",
    "    # Display fall probability and threshold\n",
    "    cv2.putText(frame, f\"Fall Prob: {fall_probability:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"Threshold: {current_threshold:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    # Display alert if fall is detected\n",
    "    if fall_detected:\n",
    "        color = (0, 0, 255)  # Red color for alert\n",
    "        cv2.putText(frame, \"FALL DETECTED!\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "        cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), color, 4)\n",
    "        \n",
    "        # Reset fall detection after 3 seconds\n",
    "        if time.time() - fall_start_time > 3:\n",
    "            fall_detected = False\n",
    "    \n",
    "    # Display cooldown status\n",
    "    cv2.putText(frame, f\"Cooldown: {frames_since_last_detection}/{cooldown_frames}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    # Calculate and display FPS\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Fall Detection', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chest Pain detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x416 2 Chest-Pains, 60.6ms\n",
      "Speed: 2.0ms preprocess, 60.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 39.0ms\n",
      "Speed: 2.0ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 3 Chest-Pains, 1 No-Pain, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 43.0ms\n",
      "Speed: 2.0ms preprocess, 43.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 40.1ms\n",
      "Speed: 1.0ms preprocess, 40.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 39.1ms\n",
      "Speed: 1.5ms preprocess, 39.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 80.7ms\n",
      "Speed: 1.0ms preprocess, 80.7ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.5ms\n",
      "Speed: 2.0ms preprocess, 37.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 39.4ms\n",
      "Speed: 1.0ms preprocess, 39.4ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 38.6ms\n",
      "Speed: 2.5ms preprocess, 38.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.6ms\n",
      "Speed: 1.5ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 38.1ms\n",
      "Speed: 2.0ms preprocess, 38.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 36.5ms\n",
      "Speed: 2.5ms preprocess, 36.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.7ms\n",
      "Speed: 1.0ms preprocess, 37.7ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.6ms\n",
      "Speed: 0.5ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 No-Pain, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 39.6ms\n",
      "Speed: 1.5ms preprocess, 39.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 41.6ms\n",
      "Speed: 2.0ms preprocess, 41.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 44.1ms\n",
      "Speed: 1.0ms preprocess, 44.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 36.1ms\n",
      "Speed: 2.0ms preprocess, 36.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.1ms\n",
      "Speed: 2.0ms preprocess, 37.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 36.5ms\n",
      "Speed: 1.0ms preprocess, 36.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.6ms\n",
      "Speed: 1.5ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 37.6ms\n",
      "Speed: 1.5ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 38.0ms\n",
      "Speed: 1.0ms preprocess, 38.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.7ms\n",
      "Speed: 1.0ms preprocess, 37.7ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 37.6ms\n",
      "Speed: 2.0ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 37.1ms\n",
      "Speed: 2.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 2 No-Pains, 38.7ms\n",
      "Speed: 1.0ms preprocess, 38.7ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.1ms\n",
      "Speed: 0.5ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 38.7ms\n",
      "Speed: 1.0ms preprocess, 38.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 37.1ms\n",
      "Speed: 0.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 37.6ms\n",
      "Speed: 1.5ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 39.6ms\n",
      "Speed: 1.0ms preprocess, 39.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 42.1ms\n",
      "Speed: 2.0ms preprocess, 42.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 41.6ms\n",
      "Speed: 1.0ms preprocess, 41.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 45.1ms\n",
      "Speed: 1.0ms preprocess, 45.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 42.6ms\n",
      "Speed: 1.0ms preprocess, 42.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 40.1ms\n",
      "Speed: 1.0ms preprocess, 40.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 40.3ms\n",
      "Speed: 1.0ms preprocess, 40.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 42.1ms\n",
      "Speed: 1.0ms preprocess, 42.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 42.2ms\n",
      "Speed: 1.0ms preprocess, 42.2ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 41.1ms\n",
      "Speed: 1.0ms preprocess, 41.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 40.8ms\n",
      "Speed: 1.0ms preprocess, 40.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 39.1ms\n",
      "Speed: 1.0ms preprocess, 39.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 40.1ms\n",
      "Speed: 0.5ms preprocess, 40.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 40.4ms\n",
      "Speed: 1.0ms preprocess, 40.4ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 39.6ms\n",
      "Speed: 1.0ms preprocess, 39.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 No-Pain, 40.6ms\n",
      "Speed: 1.0ms preprocess, 40.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 (no detections), 39.6ms\n",
      "Speed: 1.0ms preprocess, 39.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 36.5ms\n",
      "Speed: 1.0ms preprocess, 36.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.6ms\n",
      "Speed: 0.9ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 37.6ms\n",
      "Speed: 1.5ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 35.5ms\n",
      "Speed: 1.0ms preprocess, 35.5ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 No-Pain, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.5ms\n",
      "Speed: 1.0ms preprocess, 36.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.5ms\n",
      "Speed: 2.0ms preprocess, 36.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.1ms\n",
      "Speed: 1.5ms preprocess, 36.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.6ms\n",
      "Speed: 1.5ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 38.7ms\n",
      "Speed: 1.0ms preprocess, 38.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 37.6ms\n",
      "Speed: 1.5ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 No-Pains, 35.2ms\n",
      "Speed: 1.0ms preprocess, 35.2ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 42.2ms\n",
      "Speed: 1.0ms preprocess, 42.2ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 35.6ms\n",
      "Speed: 0.0ms preprocess, 35.6ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 38.1ms\n",
      "Speed: 2.0ms preprocess, 38.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 37.6ms\n",
      "Speed: 2.0ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 41.6ms\n",
      "Speed: 1.0ms preprocess, 41.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 38.5ms\n",
      "Speed: 1.0ms preprocess, 38.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 37.5ms\n",
      "Speed: 1.0ms preprocess, 37.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 2 No-Pains, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 2 No-Pains, 36.5ms\n",
      "Speed: 1.0ms preprocess, 36.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 2 No-Pains, 37.2ms\n",
      "Speed: 2.0ms preprocess, 37.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 37.1ms\n",
      "Speed: 1.0ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 41.0ms\n",
      "Speed: 1.0ms preprocess, 41.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 1 No-Pain, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 3 No-Pains, 35.6ms\n",
      "Speed: 1.0ms preprocess, 35.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 2 No-Pains, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 4 Chest-Pains, 38.5ms\n",
      "Speed: 2.5ms preprocess, 38.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 38.6ms\n",
      "Speed: 1.0ms preprocess, 38.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 2 No-Pains, 38.8ms\n",
      "Speed: 1.0ms preprocess, 38.8ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 36.6ms\n",
      "Speed: 1.0ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 2 No-Pains, 37.7ms\n",
      "Speed: 1.0ms preprocess, 37.7ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 36.1ms\n",
      "Speed: 1.0ms preprocess, 36.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 2 Chest-Pains, 1 No-Pain, 37.1ms\n",
      "Speed: 1.5ms preprocess, 37.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 35.5ms\n",
      "Speed: 2.0ms preprocess, 35.5ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 37.6ms\n",
      "Speed: 1.0ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n",
      "\n",
      "0: 224x416 1 Chest-Pain, 2 No-Pains, 36.5ms\n",
      "Speed: 1.0ms preprocess, 36.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 416)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('best_chest.pt')\n",
    "\n",
    "# Define the input and output video paths\n",
    "input_video_path = 'vids\\chest pain.mp4'\n",
    "output_video_path = 'output_chest-pain.avi'\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get the video writer initialized to save the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    if frame_count % 3 != 0:\n",
    "        # Skip frames that are not the third frame\n",
    "        continue\n",
    "\n",
    "    # Perform object detection\n",
    "    results = model(frame)\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy  # Bounding box coordinates\n",
    "        confs = result.boxes.conf  # Confidence scores\n",
    "        classes = result.boxes.cls  # Class labels\n",
    "\n",
    "        for box, conf, cls in zip(boxes, confs, classes):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            label = int(cls)\n",
    "            confidence = float(conf)\n",
    "\n",
    "            # Draw the bounding box and label on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'{label} {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
